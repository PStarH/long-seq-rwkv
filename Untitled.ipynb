{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2hiZEN-YOR6",
        "outputId": "ad7962d4-837a-4fd3-ef76-41b71b077d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=85055ba17e9a1b025252bf3b6c58aeccf56c10897e8476faa019a9f0ac0da274\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "正在加载模型与分词器...\n",
            "模型加载完成。\n",
            "初始化增强模块和消融版本...\n",
            "模块初始化完成。\n",
            "开始运行前向传播 benchmark...\n",
            "前向传播耗时 (平均每次):\n",
            "  基础模型: 0.398281 s\n",
            "  增强模型: 0.417685 s\n",
            "  消融 - 无层归一化: 0.435088 s\n",
            "  消融 - 固定门控: 0.433241 s\n",
            "生成文本耗时 (基础模型): 1.917742 s\n",
            "\n",
            "生成文本示例:\n",
            "Once upon a time, in a distant land, Once upon a time, in a distant land, Once upon a time, in a distant land, Once upon a time, in a distant land, Once upon a time, in a distant land, ���\n",
            "\n",
            "长文本生成示例:\n",
            "In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, In the realm of artificial intelligence, ���\n",
            "\n",
            "生成质量评估:\n",
            "  基础模型困惑度: 3.353\n",
            "  增强模型困惑度: 3.353\n",
            "  基础模型 BLEU: 0.171\n",
            "  增强模型 BLEU: 0.167\n",
            "  基础模型 ROUGE: {'rouge1': Score(precision=0.225, recall=0.47368421052631576, fmeasure=0.30508474576271183), 'rougeL': Score(precision=0.225, recall=0.47368421052631576, fmeasure=0.30508474576271183)}\n",
            "  增强模型 ROUGE: {'rouge1': Score(precision=0.225, recall=0.47368421052631576, fmeasure=0.30508474576271183), 'rougeL': Score(precision=0.225, recall=0.47368421052631576, fmeasure=0.30508474576271183)}\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math, torch, torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "# 尝试导入 BLEU 和 ROUGE 计算工具\n",
        "try:\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "except ImportError:\n",
        "    print(\"请安装 nltk: pip install nltk\")\n",
        "\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except ImportError:\n",
        "    print(\"请安装 rouge_score: pip install rouge_score\")\n",
        "\n",
        "# ---------------------------\n",
        "# 1. 加载模型与分词器\n",
        "# ---------------------------\n",
        "# 使用 RWKV/v6-Finch-1B6-HF，传入 trust_remote_code=True 以运行自定义代码\n",
        "model_name = \"RWKV/v6-Finch-1B6-HF\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "base_model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. 定义增强模块及消融变体（支持 loss 计算，并添加 generate 方法）\n",
        "# ---------------------------\n",
        "class EnhancedAdaptiveTokenShiftRWKV(nn.Module):\n",
        "    \"\"\"\n",
        "    完整增强版：针对每个隐藏通道采用独立门控参数，\n",
        "    融合当前与前一时刻的隐藏状态，最后进行层归一化。\n",
        "    当传入 labels 时，直接调用基础模型的 forward（支持 loss 计算）。\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        hidden_size = base_model.config.hidden_size\n",
        "        self.gate_weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.gate_bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        if \"labels\" in kwargs:\n",
        "            return self.base_model(input_ids, **kwargs)\n",
        "        kwargs.pop(\"output_hidden_states\", None)\n",
        "        outputs = self.base_model(\n",
        "            input_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "            **kwargs\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1]  # (batch, seq, hidden_size)\n",
        "        # 计算门控系数，扩展为 (1, 1, hidden_size)\n",
        "        gate = torch.sigmoid(self.gate_weight + self.gate_bias).unsqueeze(0).unsqueeze(0)\n",
        "        # 构造前一时刻隐藏状态的偏移（从第二个 token 开始）\n",
        "        shifted_hidden = torch.zeros_like(hidden_states)\n",
        "        shifted_hidden[:, 1:, :] = hidden_states[:, :-1, :] * gate\n",
        "        # 加权融合当前状态与偏移状态\n",
        "        adaptive_hidden = (1 - gate) * hidden_states + gate * shifted_hidden\n",
        "        # 层归一化\n",
        "        adaptive_hidden = nn.functional.layer_norm(adaptive_hidden, normalized_shape=(hidden_states.size(-1),))\n",
        "        return adaptive_hidden\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        # 直接调用底层模型的 generate 方法\n",
        "        return self.base_model.generate(*args, **kwargs)\n",
        "\n",
        "class AblationNoLayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    消融版本1：去掉层归一化。\n",
        "    当传入 labels 时，直接调用基础模型的 forward。\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        hidden_size = base_model.config.hidden_size\n",
        "        self.gate_weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.gate_bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        if \"labels\" in kwargs:\n",
        "            return self.base_model(input_ids, **kwargs)\n",
        "        kwargs.pop(\"output_hidden_states\", None)\n",
        "        outputs = self.base_model(\n",
        "            input_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "            **kwargs\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        gate = torch.sigmoid(self.gate_weight + self.gate_bias).unsqueeze(0).unsqueeze(0)\n",
        "        shifted_hidden = torch.zeros_like(hidden_states)\n",
        "        shifted_hidden[:, 1:, :] = hidden_states[:, :-1, :] * gate\n",
        "        adaptive_hidden = (1 - gate) * hidden_states + gate * shifted_hidden\n",
        "        return adaptive_hidden\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.base_model.generate(*args, **kwargs)\n",
        "\n",
        "class AblationFixedGate(nn.Module):\n",
        "    \"\"\"\n",
        "    消融版本2：固定门控（例如设为常数1，不进行学习）。\n",
        "    当传入 labels 时，直接调用基础模型的 forward。\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.fixed_gate = 1.0  # 固定门控值\n",
        "\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "        if \"labels\" in kwargs:\n",
        "            return self.base_model(input_ids, **kwargs)\n",
        "        kwargs.pop(\"output_hidden_states\", None)\n",
        "        outputs = self.base_model(\n",
        "            input_ids,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "            **kwargs\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        gate = torch.tensor(self.fixed_gate, device=hidden_states.device).unsqueeze(0).unsqueeze(0)\n",
        "        shifted_hidden = torch.zeros_like(hidden_states)\n",
        "        shifted_hidden[:, 1:, :] = hidden_states[:, :-1, :] * gate\n",
        "        adaptive_hidden = (1 - gate) * hidden_states + gate * shifted_hidden\n",
        "        adaptive_hidden = nn.functional.layer_norm(adaptive_hidden, normalized_shape=(hidden_states.size(-1),))\n",
        "        return adaptive_hidden\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self.base_model.generate(*args, **kwargs)\n",
        "\n",
        "# 初始化各个版本\n",
        "enhanced_model = EnhancedAdaptiveTokenShiftRWKV(base_model).to(device).eval()\n",
        "ablation_no_ln = AblationNoLayerNorm(base_model).to(device).eval()\n",
        "ablation_fixed_gate = AblationFixedGate(base_model).to(device).eval()\n",
        "\n",
        "# ---------------------------\n",
        "# 3. 定义评估函数\n",
        "# ---------------------------\n",
        "def benchmark_forward(model, inputs, num_runs=20, warmup=5):\n",
        "    for _ in range(warmup):\n",
        "        _ = model(inputs[\"input_ids\"], output_hidden_states=True)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = model(inputs[\"input_ids\"], output_hidden_states=True)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    return (end - start) / num_runs\n",
        "\n",
        "def benchmark_generate(model, inputs, max_length=50, num_runs=10, warmup=3):\n",
        "    # 使用修正的生成参数，只传入 max_new_tokens，并传入 attention_mask\n",
        "    for _ in range(warmup):\n",
        "        _ = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\"),\n",
        "            max_new_tokens=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs.get(\"attention_mask\"),\n",
        "            max_new_tokens=max_length,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    return (end - start) / num_runs\n",
        "\n",
        "def compute_perplexity(model, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    # 如果返回对象含有 loss，则直接使用\n",
        "    if hasattr(outputs, \"loss\"):\n",
        "        loss = outputs.loss.item()\n",
        "    else:\n",
        "        # 假设 outputs 为 logits，手动计算交叉熵\n",
        "        logits = outputs\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = inputs[\"input_ids\"][..., 1:].contiguous()\n",
        "        loss = nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                                           shift_labels.view(-1)).item()\n",
        "    perplexity = math.exp(loss)\n",
        "    return perplexity\n",
        "\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    reference_tokens = reference.split()\n",
        "    hypothesis_tokens = hypothesis.split()\n",
        "    return sentence_bleu([reference_tokens], hypothesis_tokens)\n",
        "\n",
        "def compute_rouge(reference, hypothesis):\n",
        "    try:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "        scores = scorer.score(reference, hypothesis)\n",
        "        return scores\n",
        "    except Exception as e:\n",
        "        print(\"ROUGE计算错误:\", e)\n",
        "        return None\n",
        "\n",
        "# ---------------------------\n",
        "# 4. 运行评估实验\n",
        "# ---------------------------\n",
        "prompt = \"Once upon a time, in a distant land, \" * 5  # 中等长度提示\n",
        "long_prompt = \"In the realm of artificial intelligence, \" * 50  # 较长文本提示\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Benchmark 前向传播时间\n",
        "base_fwd_time = benchmark_forward(base_model, inputs)\n",
        "enhanced_fwd_time = benchmark_forward(enhanced_model, inputs)\n",
        "ablation_no_ln_time = benchmark_forward(ablation_no_ln, inputs)\n",
        "ablation_fixed_gate_time = benchmark_forward(ablation_fixed_gate, inputs)\n",
        "\n",
        "# Benchmark 生成速度（使用基础模型）\n",
        "gen_time = benchmark_generate(base_model, inputs, max_length=50)\n",
        "\n",
        "print(\"前向传播耗时 (平均每次):\")\n",
        "print(\"  基础模型: {:.6f} s\".format(base_fwd_time))\n",
        "print(\"  增强模型: {:.6f} s\".format(enhanced_fwd_time))\n",
        "print(\"  消融 - 无层归一化: {:.6f} s\".format(ablation_no_ln_time))\n",
        "print(\"  消融 - 固定门控: {:.6f} s\".format(ablation_fixed_gate_time))\n",
        "print(\"生成文本耗时 (基础模型): {:.6f} s\".format(gen_time))\n",
        "\n",
        "# 生成文本示例（修复乱码问题）\n",
        "generated_ids = base_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs.get(\"attention_mask\"),\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True, errors='replace')\n",
        "print(\"\\n生成文本示例:\")\n",
        "print(generated_text)\n",
        "\n",
        "# 长文本生成测试\n",
        "long_inputs = tokenizer(long_prompt, return_tensors=\"pt\")\n",
        "long_inputs = {k: v.to(device) for k, v in long_inputs.items()}\n",
        "long_generated_ids = base_model.generate(\n",
        "    long_inputs[\"input_ids\"],\n",
        "    attention_mask=long_inputs.get(\"attention_mask\"),\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "long_generated_text = tokenizer.decode(long_generated_ids[0], skip_special_tokens=True, errors='replace')\n",
        "print(\"\\n长文本生成示例:\")\n",
        "print(long_generated_text)\n",
        "\n",
        "# 生成质量评估：困惑度、BLEU、ROUGE\n",
        "reference_text = \"Once upon a time, in a distant land, there was a wise king who ruled with kindness and justice.\"\n",
        "baseline_perplexity = compute_perplexity(base_model, reference_text)\n",
        "enhanced_perplexity = compute_perplexity(enhanced_model, reference_text)\n",
        "baseline_generated = tokenizer.decode(\n",
        "    base_model.generate(inputs[\"input_ids\"], attention_mask=inputs.get(\"attention_mask\"), max_new_tokens=50,\n",
        "                        do_sample=True, temperature=0.8, top_p=0.95, repetition_penalty=1.1)[0],\n",
        "    skip_special_tokens=True, errors='replace'\n",
        ")\n",
        "enhanced_generated = tokenizer.decode(\n",
        "    enhanced_model.generate(inputs[\"input_ids\"], attention_mask=inputs.get(\"attention_mask\"), max_new_tokens=50,\n",
        "                              do_sample=True, temperature=0.8, top_p=0.95, repetition_penalty=1.1)[0],\n",
        "    skip_special_tokens=True, errors='replace'\n",
        ")\n",
        "bleu_baseline = compute_bleu(reference_text, baseline_generated)\n",
        "bleu_enhanced = compute_bleu(reference_text, enhanced_generated)\n",
        "rouge_baseline = compute_rouge(reference_text, baseline_generated)\n",
        "rouge_enhanced = compute_rouge(reference_text, enhanced_generated)\n",
        "\n",
        "print(\"\\n生成质量评估:\")\n",
        "print(\"  基础模型困惑度: {:.3f}\".format(baseline_perplexity))\n",
        "print(\"  增强模型困惑度: {:.3f}\".format(enhanced_perplexity))\n",
        "print(\"  基础模型 BLEU: {:.3f}\".format(bleu_baseline))\n",
        "print(\"  增强模型 BLEU: {:.3f}\".format(bleu_enhanced))\n",
        "print(\"  基础模型 ROUGE: {}\".format(rouge_baseline))\n",
        "print(\"  增强模型 ROUGE: {}\".format(rouge_enhanced))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "6FPnQssJaT53",
        "outputId": "f0ac31c9-5959-438f-86ca-ea6379c4030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 4582 has 14.72 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 1.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-557aed693780>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m                 )\n\u001b[0;32m-> 3110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 4582 has 14.72 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 1.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}